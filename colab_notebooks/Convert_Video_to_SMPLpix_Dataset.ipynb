{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert Video to SMPLpix Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cbXoNhFF-D8Q"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saNBv0dY-Eef"
      },
      "source": [
        "![](https://user-images.githubusercontent.com/8117267/116876711-8defc300-ac25-11eb-8b7b-5eab8860602c.png)\n",
        "\n",
        "# SMPLpix Dataset Preparation.\n",
        "\n",
        "**Author**: [Sergey Prokudin](https://ps.is.mpg.de/people/sprokudin). \n",
        "[[Project Page](https://sergeyprokudin.github.io/smplpix/)]\n",
        "[[Paper](https://arxiv.org/pdf/2008.06872.pdf)]\n",
        "[[Video](https://www.youtube.com/watch?v=JY9t4xUAouk)]\n",
        "[[GitHub](https://github.com/sergeyprokudin/smplpix)]\n",
        "\n",
        "This notebook contains an example workflow for converting a video file to a SMPLpix dataset. \n",
        "### Processing steps:\n",
        "\n",
        "1. Download the video of choice, extract frames;\n",
        "2. Extract **2D keypoints**: run [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) on the extracted frames;\n",
        "3. Infer **3D human meshes**: run [SMPLify-x](https://github.com/vchoutas/smplify-x) on the extracted frames + keypoints;\n",
        "4. Form dataset **image pairs**, where *input* is SMPL-X mesh render, and *output* is the corresponding target ground truth video frame;\n",
        "5. **Split the data** into train, test and validation, zip and copy to Google Drive.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Convert a video into our dataset format using this notebook.\n",
        "2. Train a SMPLpix using the training notebook.\n",
        "\n",
        "\n",
        "### Notes\n",
        "* While this will work for small datasets in a Colab runtime, larger datasets will require more compute power;\n",
        "* If you would like to train a model on a serious dataset, you should consider copying this to your own workstation and running it there. Some minor modifications will be required, and you will have to install the dependencies separately;\n",
        "* Please report issues on the [GitHub issue tracker](https://github.com/sergeyprokudin/smplpix/issues).\n",
        "\n",
        "If you find this work useful, please consider citing:\n",
        "```bibtex\n",
        "@inproceedings{prokudin2021smplpix,\n",
        "  title={SMPLpix: Neural Avatars from 3D Human Models},\n",
        "  author={Prokudin, Sergey and Black, Michael J and Romero, Javier},\n",
        "  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},\n",
        "  pages={1810--1819},\n",
        "  year={2021}\n",
        "}\n",
        "```\n",
        "\n",
        "Many thanks [Keunhong Park](https://keunhong.com) for providing the [Nerfie dataset preparation template](https://colab.research.google.com/github/google/nerfies/blob/main/notebooks/Nerfies_Capture_Processing.ipynb)!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXoNhFF-D8Q"
      },
      "source": [
        "## Upload the video and extract frames\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0EmRUXf5nTr",
        "cellView": "form"
      },
      "source": [
        "# @title Upload a video file (.mp4, .mov, etc.) from your disk, Dropbox, Google Drive or YouTube\n",
        "\n",
        "# @markdown  This will upload it to the local Colab working directory. You can use a demo video to test the pipeline. The background in the demo was removed with the [Unscreen](https://www.unscreen.com/) service. Alternatively, you can try [PointRend](https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend) segmentation for this purpose. \n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def download_youtube_video(img_url, save_path, resolution_id=-3):\n",
        "\n",
        "  print(\"downloading the video: %s\" % img_url)\n",
        "  res_path = YouTube(img_url).streams.order_by('resolution')[resolution_id].download(save_path)\n",
        "\n",
        "  return res_path\n",
        "\n",
        "def download_dropbox_url(url, filepath, chunk_size=1024):\n",
        "\n",
        "    import requests\n",
        "    headers = {'user-agent': 'Wget/1.16 (linux-gnu)'}\n",
        "    r = requests.get(url, stream=True, headers=headers)\n",
        "    with open(filepath, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    return filepath\n",
        "\n",
        "!rm -rf /content/data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "VIDEO_SOURCE = 'dropbox' #@param [\"youtube\", \"upload\", \"google drive\", \"dropbox\"]\n",
        "\n",
        "if VIDEO_SOURCE == 'dropbox':\n",
        "  DROPBOX_URL  = 'https://www.dropbox.com/s/rjqwf894ovso218/smplpix_test_video_na.mp4?dl=0' #@param \n",
        "  VIDEO_PATH  = '/content/video.mp4' \n",
        "  download_dropbox_url(DROPBOX_URL, VIDEO_PATH)\n",
        "elif VIDEO_SOURCE == 'upload':\n",
        "  print(\"Please upload the video: \")\n",
        "  uploaded = files.upload()\n",
        "  VIDEO_PATH = os.path.join('/content', list(uploaded.keys())[0])\n",
        "elif VIDEO_SOURCE == 'youtube':\n",
        "  !pip install pytube\n",
        "  from pytube import YouTube\n",
        "  YOTUBE_VIDEO_URL  = ''  #@param \n",
        "  VIDEO_PATH = download_youtube_video(YOTUBE_VIDEO_URL, '/content/')\n",
        "elif VIDEO_SOURCE == 'google drive':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  GOOGLE_DRIVE_PATH = '' #@param \n",
        "  VIDEO_PATH  = GOOGLE_DRIVE_PATH\n",
        "\n",
        "\n",
        "print(\"video is uploaded to %s\" % VIDEO_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VImnPzFg9UNA",
        "cellView": "form"
      },
      "source": [
        "# @title Flatten the video into frames\n",
        "\n",
        "\n",
        "FPS =     1# @param {type:'number'}\n",
        "\n",
        "# @markdown _Note_: for longer videos, it might make sense to decrease the FPS as it will take 30-60 seconds for SMPLify-X framework to process every frame.\n",
        "\n",
        "\n",
        "\n",
        "RES_DIR = '/content/data'\n",
        "FRAMES_DIR = os.path.join(RES_DIR, 'images')\n",
        "!rm -rf $RES_DIR\n",
        "!mkdir $RES_DIR\n",
        "!mkdir $FRAMES_DIR\n",
        "!ffmpeg -i \"$VIDEO_PATH\" -vf fps=$FPS  -qscale:v 2 '$FRAMES_DIR/%05d.png'\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_img(img_path):\n",
        "\n",
        "  return np.asarray(Image.open(img_path))/255\n",
        "\n",
        "test_img_path = os.path.join(FRAMES_DIR, os.listdir(FRAMES_DIR)[0])\n",
        "\n",
        "test_img = load_img(test_img_path)\n",
        "\n",
        "plt.figure(figsize=(5, 10))\n",
        "plt.title(\"extracted image example\")\n",
        "plt.imshow(test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z-ASlgBUPXJ"
      },
      "source": [
        "## Extract 2D body keypoints with OpenPose\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AL4QpsBUO9p",
        "cellView": "form"
      },
      "source": [
        "# @title Install OpenPose\n",
        "# @markdown This will take some time (~10 mins). The code is taken from this [OpenPose Colab notebook](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/OpenPose.ipynb).\n",
        "\n",
        "%cd /content\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  # install python dependencies\n",
        "  !pip install -q youtube-dl\n",
        "  # build openpose\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. && make -j`nproc`\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NR5OGyeUOKU",
        "cellView": "form"
      },
      "source": [
        "# @title Run OpenPose on the extracted frames\n",
        "%cd /content\n",
        "KEYPOINTS_DIR = os.path.join(RES_DIR, 'keypoints')\n",
        "OPENPOSE_IMAGES_DIR = os.path.join(RES_DIR, 'openpose_images')\n",
        "!mkdir $KEYPOINTS_DIR\n",
        "!mkdir $OPENPOSE_IMAGES_DIR\n",
        "\n",
        "!cd openpose && ./build/examples/openpose/openpose.bin --image_dir $FRAMES_DIR --write_json $KEYPOINTS_DIR --face --hand --display 0   --write_images $OPENPOSE_IMAGES_DIR\n",
        "\n",
        "input_img_path = os.path.join(FRAMES_DIR, sorted(os.listdir(FRAMES_DIR))[0])\n",
        "openpose_img_path = os.path.join(OPENPOSE_IMAGES_DIR, sorted(os.listdir(OPENPOSE_IMAGES_DIR))[0])\n",
        "\n",
        "test_img = load_img(input_img_path)\n",
        "open_pose_img = load_img(openpose_img_path)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title(\"Input Frame + Openpose Prediction\")\n",
        "plt.imshow(np.concatenate([test_img, open_pose_img], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to4QpKLFHf2s"
      },
      "source": [
        "\n",
        "## Infer 3D Human Model with [SMPLify-X](https://smpl-x.is.tue.mpg.de/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFzPpUoM99nd",
        "cellView": "form"
      },
      "source": [
        "# @title Install SMPLify-X and other dependencies\n",
        "\n",
        "%cd /content\n",
        "!pip install chumpy\n",
        "!pip install smplx\n",
        "!git clone https://github.com/vchoutas/smplx\n",
        "%cd smplx\n",
        "!python setup.py install\n",
        "\n",
        "#vposer\n",
        "!pip install git+https://github.com/nghorbani/configer\n",
        "!pip install git+https://github.com/sergeyprokudin/human_body_prior\n",
        "\n",
        "!pip install torch==1.1.0\n",
        "%cd /content\n",
        "!git clone https://github.com/sergeyprokudin/smplify-x\n",
        "%cd /content/smplify-x\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSPgjzkFURq-",
        "cellView": "form"
      },
      "source": [
        "# @title Upload the SMPL-X model files\n",
        "\n",
        "# @markdown Proceed to the [official website](https://smpl-x.is.tue.mpg.de/), register and download the zip files with SMPL-X (**models_smplx_v1_1.zip**, ~830MB) and VPoser (**vposer_v1_0.zip**, ~2.5MB) models from the **Downloads** section. \n",
        "# @markdown\n",
        "\n",
        "# @markdown Since uploading large zip files to Colab is relatively slow, we expect you to upload these files to Google Drive instead, link gdrive to the Colab file systems and modify **SMPLX_ZIP_PATH** and **VPOSER_ZIP_PATH** variables accordingly.\n",
        "\n",
        "%cd /content/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "SMPLX_ZIP_PATH = '/content/gdrive/MyDrive/datasets/models_smplx_v1_1.zip' # @param {type:\"string\"}\n",
        "VPOSER_ZIP_PATH = '/content/gdrive/MyDrive/datasets/vposer_v1_0.zip' # @param {type:\"string\"}\n",
        "\n",
        "SMPLX_MODEL_PATH = '/content/smplx'\n",
        "!mkdir $SMPLX_MODEL_PATH\n",
        "!unzip -n '$SMPLX_ZIP_PATH' -d  $SMPLX_MODEL_PATH\n",
        "VPOSER_MODEL_PATH = '/content/vposer'\n",
        "!mkdir $VPOSER_MODEL_PATH\n",
        "!unzip -n '$VPOSER_ZIP_PATH' -d  $VPOSER_MODEL_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODeGAyGrrIov",
        "cellView": "form"
      },
      "source": [
        "# @title Run SMPLify-X\n",
        "\n",
        "# @markdown Please select gender of the SMPL-X model:\n",
        "\n",
        "gender = 'male' #@param [\"neutral\", \"female\", \"male\"]\n",
        "\n",
        "# @markdown Please keep in mind that estimating 3D body with SMPLify-X framework will take ~30-60 secs, so processing long videos at high FPS might take a long time.\n",
        "\n",
        "!rm -rf /content/data/smplifyx_results\n",
        "%cd /content/smplify-x\n",
        "!git pull origin\n",
        "!python smplifyx/main.py --config cfg_files/fit_smplx.yaml \\\n",
        "    --data_folder  /content/data \\\n",
        "    --output_folder /content/data/smplifyx_results \\\n",
        "    --visualize=True \\\n",
        "    --gender=$gender \\\n",
        "    --model_folder /content/smplx/models \\\n",
        "    --vposer_ckpt /content/vposer/vposer_v1_0 \\\n",
        "    --part_segm_fn smplx_parts_segm.pkl "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGcmsijSQ5jd",
        "cellView": "form"
      },
      "source": [
        "# @title Make train-test-validation splits, copy data to final folders\n",
        "\n",
        "import shutil\n",
        "\n",
        "train_ratio = 0.9 #@param\n",
        " \n",
        "final_zip_path = '/content/gdrive/MyDrive/datasets/smplpix_data_test.zip' # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "target_images_path = '/content/data/smplifyx_results/input_images'\n",
        "smplifyx_renders = '/content/data/smplifyx_results/rendered_smplifyx_meshes'\n",
        "\n",
        "smplpix_data_path = '/content/smplpix_data'\n",
        "\n",
        "train_input_dir = os.path.join(smplpix_data_path, 'train', 'input')\n",
        "train_output_dir = os.path.join(smplpix_data_path, 'train', 'output')\n",
        "val_input_dir = os.path.join(smplpix_data_path, 'validation', 'input')\n",
        "val_output_dir = os.path.join(smplpix_data_path, 'validation', 'output')\n",
        "test_input_dir = os.path.join(smplpix_data_path, 'test', 'input')\n",
        "test_output_dir = os.path.join(smplpix_data_path, 'test', 'output')\n",
        "\n",
        "!mkdir -p $train_input_dir\n",
        "!mkdir -p $train_output_dir\n",
        "!mkdir -p $val_input_dir\n",
        "!mkdir -p $val_output_dir\n",
        "!mkdir -p $test_input_dir\n",
        "!mkdir -p $test_output_dir\n",
        "\n",
        "img_names = sorted(os.listdir(target_images_path))\n",
        "n_images = len(img_names)\n",
        "n_train_images = int(n_images * train_ratio)\n",
        "n_val_images = int(n_images * (1-train_ratio) / 2)\n",
        "train_images = img_names[0:n_train_images]\n",
        "val_images = img_names[n_train_images:n_train_images+n_val_images]\n",
        "test_images = img_names[n_train_images:]\n",
        "\n",
        "for img in train_images:\n",
        "  shutil.copy(os.path.join(smplifyx_renders, img), train_input_dir)\n",
        "  shutil.copy(os.path.join(target_images_path, img), train_output_dir)\n",
        "\n",
        "for img in val_images:\n",
        "  shutil.copy(os.path.join(smplifyx_renders, img), val_input_dir)\n",
        "  shutil.copy(os.path.join(target_images_path, img), val_output_dir)\n",
        "\n",
        "for img in test_images:\n",
        "  shutil.copy(os.path.join(smplifyx_renders, img), test_input_dir)\n",
        "  shutil.copy(os.path.join(target_images_path, img), test_output_dir)\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!zip -r $final_zip_path smplpix_data/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
